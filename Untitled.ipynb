{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"gs_lr_tfidf\", \"rb\") as f:\n",
    "    dump = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', '...se_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([   3.67061005,  154.38343019,    6.48917112,  186.21905107,\n",
       "           4.56606102,  170.14713187,    5.2651011 ,  170.79636908,\n",
       "           6.4317678 ,  224.45783834,    9.28353105,  229.56413031,\n",
       "           6.98919988,  227.69562349,    9.39373736,  228.36106153,\n",
       "           6.15775223,  218.65550647,    8.15486636,  228.21305313,\n",
       "           7.71604142,  216.50698347,    9.76995883,  189.22702322,\n",
       "           4.89207983,  177.04872665,    6.92279587,  192.77882633,\n",
       "          10.23758569,  231.90166397,   19.46891351,  230.92280812,\n",
       "           5.83473377,  199.82182927,    7.95925531,  203.27282648,\n",
       "          11.50585814,  184.89957557,   17.90562396,  199.7092227 ,\n",
       "           4.89207978,  168.62264466,    6.3377624 ,  172.76728177,\n",
       "          10.68781128,  176.59890089,   16.68035402,  178.65981884]),\n",
       " 'mean_score_time': array([  0.67263842,  43.45068531,   1.16846685,  44.6219523 ,\n",
       "          0.84364829,  40.73732991,   1.02905898,  45.3905962 ,\n",
       "          1.09686279,  55.64418273,   1.30027432,  57.47168717,\n",
       "          1.14246526,  56.30522041,   1.31047497,  55.84419417,\n",
       "          1.03185892,  55.69458561,   1.35647759,  55.82439289,\n",
       "          1.1266644 ,  50.39888272,   1.28067331,  44.87556672,\n",
       "          0.93265343,  43.59029326,   1.01885829,  48.01354623,\n",
       "          1.02305837,  57.27847619,   1.310075  ,  52.1973855 ,\n",
       "          0.9508544 ,  49.11720924,   1.17926745,  47.20950031,\n",
       "          0.90925207,  43.30807705,   1.17946749,  46.03243294,\n",
       "          0.79264536,  40.91834044,   0.95425463,  42.1916132 ,\n",
       "          0.88545065,  42.94745646,   1.10306311,  38.88682413]),\n",
       " 'mean_test_score': array([ 0.86932707,  0.86883803,  0.87142997,  0.87045188,  0.88434077,\n",
       "         0.88326487,  0.88492762,  0.88321596,  0.86966941,  0.86800665,\n",
       "         0.87573357,  0.87235915,  0.88918232,  0.88355829,  0.89152973,\n",
       "         0.88590571,  0.86517019,  0.85768779,  0.86830008,  0.86145344,\n",
       "         0.88331377,  0.8750489 ,  0.88497653,  0.8771518 ,  0.86771322,\n",
       "         0.86575704,  0.87216354,  0.86893584,  0.87539124,  0.86776213,\n",
       "         0.87808099,  0.87089202,  0.85832355,  0.85294405,  0.86389867,\n",
       "         0.85646518,  0.86966941,  0.85900822,  0.87142997,  0.86389867,\n",
       "         0.85788341,  0.8481025 ,  0.86135563,  0.85294405,  0.86458333,\n",
       "         0.85519366,  0.86722418,  0.85832355]),\n",
       " 'mean_train_score': array([ 0.89014822,  0.89013596,  0.88997705,  0.89088177,  0.94180361,\n",
       "         0.93149694,  0.93555607,  0.9272545 ,  0.99889964,  0.99743251,\n",
       "         0.99784819,  0.99620989,  0.9943393 ,  0.98807952,  0.99149061,\n",
       "         0.98487628,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99998777,  0.98877639,\n",
       "         0.98383708,  0.98999902,  0.98618445,  0.99910749,  0.99776262,\n",
       "         0.99932756,  0.99825166,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'param_clf__C': masked_array(data = [1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 10.0 10.0 10.0 10.0 10.0 10.0 10.0 10.0\n",
       "  100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
       "  1.0 1.0 10.0 10.0 10.0 10.0 10.0 10.0 10.0 10.0 100.0 100.0 100.0 100.0\n",
       "  100.0 100.0 100.0 100.0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_clf__penalty': masked_array(data = ['l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2' 'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2'\n",
       "  'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2' 'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2'\n",
       "  'l2' 'l2' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2' 'l2' 'l1' 'l1' 'l1' 'l1' 'l2'\n",
       "  'l2' 'l2' 'l2'],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__ngram_range': masked_array(data = [(1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1)\n",
       "  (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1)\n",
       "  (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1)\n",
       "  (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1)\n",
       "  (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1) (1, 1)],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__norm': masked_array(data = [-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
       "  None None None None None None None None None None None None None None None\n",
       "  None None None None None None None None None],\n",
       "              mask = [ True  True  True  True  True  True  True  True  True  True  True  True\n",
       "   True  True  True  True  True  True  True  True  True  True  True  True\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__stop_words': masked_array(data = [ ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
       "  None None],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__tokenizer': masked_array(data = [<function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>\n",
       "  <function tokenizer at 0x00000000088F29D8>\n",
       "  <function tokenizer_porter at 0x000000000868FD08>],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_vect__use_idf': masked_array(data = [-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "              mask = [ True  True  True  True  True  True  True  True  True  True  True  True\n",
       "   True  True  True  True  True  True  True  True  True  True  True  True\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 1.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 10.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l1',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': ['i',\n",
       "    'me',\n",
       "    'my',\n",
       "    'myself',\n",
       "    'we',\n",
       "    'our',\n",
       "    'ours',\n",
       "    'ourselves',\n",
       "    'you',\n",
       "    'your',\n",
       "    'yours',\n",
       "    'yourself',\n",
       "    'yourselves',\n",
       "    'he',\n",
       "    'him',\n",
       "    'his',\n",
       "    'himself',\n",
       "    'she',\n",
       "    'her',\n",
       "    'hers',\n",
       "    'herself',\n",
       "    'it',\n",
       "    'its',\n",
       "    'itself',\n",
       "    'they',\n",
       "    'them',\n",
       "    'their',\n",
       "    'theirs',\n",
       "    'themselves',\n",
       "    'what',\n",
       "    'which',\n",
       "    'who',\n",
       "    'whom',\n",
       "    'this',\n",
       "    'that',\n",
       "    'these',\n",
       "    'those',\n",
       "    'am',\n",
       "    'is',\n",
       "    'are',\n",
       "    'was',\n",
       "    'were',\n",
       "    'be',\n",
       "    'been',\n",
       "    'being',\n",
       "    'have',\n",
       "    'has',\n",
       "    'had',\n",
       "    'having',\n",
       "    'do',\n",
       "    'does',\n",
       "    'did',\n",
       "    'doing',\n",
       "    'a',\n",
       "    'an',\n",
       "    'the',\n",
       "    'and',\n",
       "    'but',\n",
       "    'if',\n",
       "    'or',\n",
       "    'because',\n",
       "    'as',\n",
       "    'until',\n",
       "    'while',\n",
       "    'of',\n",
       "    'at',\n",
       "    'by',\n",
       "    'for',\n",
       "    'with',\n",
       "    'about',\n",
       "    'against',\n",
       "    'between',\n",
       "    'into',\n",
       "    'through',\n",
       "    'during',\n",
       "    'before',\n",
       "    'after',\n",
       "    'above',\n",
       "    'below',\n",
       "    'to',\n",
       "    'from',\n",
       "    'up',\n",
       "    'down',\n",
       "    'in',\n",
       "    'out',\n",
       "    'on',\n",
       "    'off',\n",
       "    'over',\n",
       "    'under',\n",
       "    'again',\n",
       "    'further',\n",
       "    'then',\n",
       "    'once',\n",
       "    'here',\n",
       "    'there',\n",
       "    'when',\n",
       "    'where',\n",
       "    'why',\n",
       "    'how',\n",
       "    'all',\n",
       "    'any',\n",
       "    'both',\n",
       "    'each',\n",
       "    'few',\n",
       "    'more',\n",
       "    'most',\n",
       "    'other',\n",
       "    'some',\n",
       "    'such',\n",
       "    'no',\n",
       "    'nor',\n",
       "    'not',\n",
       "    'only',\n",
       "    'own',\n",
       "    'same',\n",
       "    'so',\n",
       "    'than',\n",
       "    'too',\n",
       "    'very',\n",
       "    's',\n",
       "    't',\n",
       "    'can',\n",
       "    'will',\n",
       "    'just',\n",
       "    'don',\n",
       "    'should',\n",
       "    'now',\n",
       "    'd',\n",
       "    'll',\n",
       "    'm',\n",
       "    'o',\n",
       "    're',\n",
       "    've',\n",
       "    'y',\n",
       "    'ain',\n",
       "    'aren',\n",
       "    'couldn',\n",
       "    'didn',\n",
       "    'doesn',\n",
       "    'hadn',\n",
       "    'hasn',\n",
       "    'haven',\n",
       "    'isn',\n",
       "    'ma',\n",
       "    'mightn',\n",
       "    'mustn',\n",
       "    'needn',\n",
       "    'shan',\n",
       "    'shouldn',\n",
       "    'wasn',\n",
       "    'weren',\n",
       "    'won',\n",
       "    'wouldn'],\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer>,\n",
       "   'vect__use_idf': False},\n",
       "  {'clf__C': 100.0,\n",
       "   'clf__penalty': 'l2',\n",
       "   'vect__ngram_range': (1, 1),\n",
       "   'vect__norm': None,\n",
       "   'vect__stop_words': None,\n",
       "   'vect__tokenizer': <function tokenizers.tokenizer_porter>,\n",
       "   'vect__use_idf': False}),\n",
       " 'rank_test_score': array([24, 26, 18, 21,  6,  9,  5, 10, 22, 28, 13, 16,  2,  7,  1,  3, 33,\n",
       "        43, 27, 37,  8, 15,  4, 12, 30, 32, 17, 25, 14, 29, 11, 20, 40, 46,\n",
       "        35, 44, 22, 39, 18, 35, 42, 48, 38, 46, 34, 45, 31, 40]),\n",
       " 'split0_test_score': array([ 0.86821516,  0.87017115,  0.86528117,  0.86943765,  0.87897311,\n",
       "         0.87897311,  0.87823961,  0.88117359,  0.86503667,  0.86479218,\n",
       "         0.87261614,  0.86943765,  0.88533007,  0.88239609,  0.88655257,\n",
       "         0.88166259,  0.85892421,  0.85599022,  0.86405868,  0.85794621,\n",
       "         0.88141809,  0.87481663,  0.8799511 ,  0.87457213,  0.86601467,\n",
       "         0.86503667,  0.87163814,  0.86699267,  0.87603912,  0.86870416,\n",
       "         0.87555012,  0.86674817,  0.85525672,  0.85232274,  0.86356968,\n",
       "         0.85403423,  0.87090465,  0.8594132 ,  0.86943765,  0.8603912 ,\n",
       "         0.85183374,  0.85012225,  0.8603912 ,  0.85012225,  0.86674817,\n",
       "         0.85427873,  0.86283619,  0.85403423]),\n",
       " 'split0_train_score': array([ 0.89112361,  0.88983983,  0.89277418,  0.89228512,  0.9422301 ,\n",
       "         0.9317765 ,  0.9351999 ,  0.92725272,  0.99877736,  0.99749358,\n",
       "         0.99761585,  0.99633207,  0.99455924,  0.98838489,  0.99162489,\n",
       "         0.98496149,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.98911847,\n",
       "         0.98459469,  0.98997432,  0.98648979,  0.99908302,  0.99792151,\n",
       "         0.99932755,  0.99816603,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.87090465,  0.87359413,  0.87506112,  0.87579462,  0.88924205,\n",
       "         0.88704156,  0.89462103,  0.89242054,  0.87310513,  0.87090465,\n",
       "         0.88215159,  0.87481663,  0.88899756,  0.88679707,  0.89657702,\n",
       "         0.89511002,  0.87017115,  0.85770171,  0.87897311,  0.86479218,\n",
       "         0.88410758,  0.87628362,  0.88875306,  0.88312958,  0.87090465,\n",
       "         0.86797066,  0.87701711,  0.87383863,  0.87310513,  0.86919315,\n",
       "         0.87628362,  0.87775061,  0.85819071,  0.85476773,  0.86210269,\n",
       "         0.8596577 ,  0.87041565,  0.86161369,  0.86797066,  0.86968215,\n",
       "         0.8601467 ,  0.84596577,  0.86161369,  0.85427873,  0.86430318,\n",
       "         0.85819071,  0.86356968,  0.86259169]),\n",
       " 'split1_train_score': array([ 0.89069568,  0.88965644,  0.88959531,  0.89039002,  0.94009048,\n",
       "         0.92914782,  0.93507764,  0.92523536,  0.99877736,  0.99669886,\n",
       "         0.99816603,  0.99633207,  0.99400905,  0.98759017,  0.9912581 ,\n",
       "         0.98392224,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.98814036,\n",
       "         0.98349431,  0.99034112,  0.98508375,  0.99914415,  0.99743245,\n",
       "         0.99932755,  0.99834943,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.86674817,  0.86185819,  0.86870416,  0.86185819,  0.87799511,\n",
       "         0.8806846 ,  0.88117359,  0.8792176 ,  0.86185819,  0.86625917,\n",
       "         0.86894866,  0.86748166,  0.88386308,  0.88239609,  0.88704156,\n",
       "         0.8801956 ,  0.85696822,  0.85427873,  0.86112469,  0.85843521,\n",
       "         0.8794621 ,  0.87383863,  0.88386308,  0.87334963,  0.85745721,\n",
       "         0.86161369,  0.86797066,  0.86356968,  0.86821516,  0.87041565,\n",
       "         0.8794621 ,  0.86674817,  0.85867971,  0.85427873,  0.86210269,\n",
       "         0.85647922,  0.86479218,  0.85867971,  0.87188264,  0.86112469,\n",
       "         0.85427873,  0.84816626,  0.8591687 ,  0.85599022,  0.8596577 ,\n",
       "         0.85330073,  0.86845966,  0.85647922]),\n",
       " 'split2_train_score': array([ 0.88953417,  0.88965644,  0.88935078,  0.89100134,  0.94284142,\n",
       "         0.93318254,  0.93678934,  0.92780291,  0.99889962,  0.99773811,\n",
       "         0.99749358,  0.99541509,  0.9946815 ,  0.98826262,  0.99211395,\n",
       "         0.98538941,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99993887,  0.98899621,\n",
       "         0.98538941,  0.99040225,  0.98667319,  0.99902189,  0.99804377,\n",
       "         0.99944981,  0.99816603,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split3_test_score': array([ 0.87261614,  0.87041565,  0.87017115,  0.87139364,  0.88655257,\n",
       "         0.88606357,  0.88459658,  0.88215159,  0.87628362,  0.86528117,\n",
       "         0.87701711,  0.87432763,  0.89535452,  0.88728606,  0.89315403,\n",
       "         0.89022005,  0.87261614,  0.85892421,  0.87114914,  0.86161369,\n",
       "         0.88606357,  0.87555012,  0.88630807,  0.87823961,  0.87261614,\n",
       "         0.86772616,  0.87212714,  0.87041565,  0.8806846 ,  0.86748166,\n",
       "         0.8797066 ,  0.87383863,  0.8591687 ,  0.84767726,  0.86234719,\n",
       "         0.85378973,  0.87334963,  0.85843521,  0.87530562,  0.86503667,\n",
       "         0.85867971,  0.84596577,  0.8599022 ,  0.85036675,  0.86943765,\n",
       "         0.85599022,  0.87334963,  0.86185819]),\n",
       " 'split3_train_score': array([ 0.89100134,  0.89142927,  0.88953417,  0.89051229,  0.9424135 ,\n",
       "         0.93067612,  0.93599462,  0.92859763,  0.99902189,  0.99773811,\n",
       "         0.99779924,  0.99651547,  0.99425358,  0.9880181 ,  0.99113584,\n",
       "         0.98538941,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.98807923,\n",
       "         0.98300526,  0.98917961,  0.98587847,  0.99914415,  0.99786037,\n",
       "         0.99920528,  0.99822717,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split4_test_score': array([ 0.86815068,  0.86815068,  0.87793542,  0.87377691,  0.88894325,\n",
       "         0.88356164,  0.88600783,  0.88111546,  0.87206458,  0.87279843,\n",
       "         0.87793542,  0.87573386,  0.89236791,  0.87891389,  0.89432485,\n",
       "         0.88233855,  0.86717221,  0.86154599,  0.86619374,  0.86448141,\n",
       "         0.88551859,  0.87475538,  0.88600783,  0.87646771,  0.87157534,\n",
       "         0.86643836,  0.87206458,  0.86986301,  0.87891389,  0.8630137 ,\n",
       "         0.87940313,  0.86937378,  0.8603229 ,  0.85567515,  0.86937378,\n",
       "         0.85836595,  0.86888454,  0.85689824,  0.87255382,  0.86325832,\n",
       "         0.86448141,  0.85029354,  0.8657045 ,  0.85396282,  0.86276908,\n",
       "         0.85420744,  0.86790607,  0.85665362]),\n",
       " 'split4_train_score': array([ 0.88838631,  0.8900978 ,  0.88863081,  0.89022005,  0.94144254,\n",
       "         0.93270171,  0.93471883,  0.92738386,  0.999022  ,  0.99749389,\n",
       "         0.99816626,  0.99645477,  0.99419315,  0.98814181,  0.99132029,\n",
       "         0.98471883,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.98954768,\n",
       "         0.98270171,  0.9900978 ,  0.98679707,  0.99914425,  0.99755501,\n",
       "         0.99932763,  0.99834963,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([  0.1057993 ,  17.84794317,   0.16083508,   3.50143753,\n",
       "          0.0779158 ,   7.26465673,   0.16157814,   3.76769622,\n",
       "          0.647469  ,   2.07883261,   0.67602935,   4.08571125,\n",
       "          0.54058667,   7.63039624,   0.34475576,   2.67724187,\n",
       "          0.22493204,   5.03029878,   0.10718377,   1.09491907,\n",
       "          0.51238488,   9.50066748,   0.18932604,   8.87972653,\n",
       "          0.45119604,   1.18050187,   0.58704452,  16.78175363,\n",
       "          1.30318232,   6.01093753,   3.23654473,   1.3739063 ,\n",
       "          0.14254878,   3.56456874,   0.49008208,   4.94069394,\n",
       "          2.00955237,  11.82245821,   1.3266634 ,  13.01503568,\n",
       "          0.29376784,   2.15402316,   0.45298443,   5.36357037,\n",
       "          0.80652127,   6.98384288,   1.3300711 ,  15.82084501]),\n",
       " 'std_score_time': array([ 0.0413381 ,  2.25741112,  0.05233101,  2.4619073 ,  0.03559191,\n",
       "         0.50723281,  0.02164387,  3.18638664,  0.0223207 ,  0.37939332,\n",
       "         0.05592594,  2.34514075,  0.06182457,  2.53391481,  0.11213786,\n",
       "         0.98358762,  0.04600871,  0.67171407,  0.03122739,  1.02707848,\n",
       "         0.13022508,  2.25141746,  0.04728312,  3.12678704,  0.13226883,\n",
       "         0.44025854,  0.03338695,  5.75882809,  0.07862247,  4.12762062,\n",
       "         0.18486626,  0.82234252,  0.05885029,  0.92675286,  0.07649595,\n",
       "         3.9181689 ,  0.08931665,  3.5393734 ,  0.26275257,  5.89665601,\n",
       "         0.05159591,  0.27473381,  0.04501544,  1.32488069,  0.04453838,\n",
       "         3.12979532,  0.12676854,  6.96066974]),\n",
       " 'std_test_score': array([ 0.00212505,  0.00390088,  0.00452367,  0.00480362,  0.00488217,\n",
       "         0.0030746 ,  0.00555113,  0.0046997 ,  0.00536241,  0.00323011,\n",
       "         0.00454858,  0.00327498,  0.00427589,  0.00311936,  0.0040211 ,\n",
       "         0.00577368,  0.00617675,  0.00248652,  0.00626035,  0.00288969,\n",
       "         0.00250928,  0.0008223 ,  0.00295287,  0.00342058,  0.00560747,\n",
       "         0.00232079,  0.00287849,  0.00345438,  0.00441756,  0.00255323,\n",
       "         0.0017852 ,  0.00429834,  0.00168885,  0.00285277,  0.00279043,\n",
       "         0.00231835,  0.00282962,  0.00153882,  0.00254666,  0.00332133,\n",
       "         0.00444681,  0.00189814,  0.00231508,  0.00231108,  0.00334325,\n",
       "         0.00173267,  0.00379814,  0.00332575]),\n",
       " 'std_train_score': array([  1.04501777e-03,   6.66585653e-04,   1.44016544e-03,\n",
       "          7.48317788e-04,   9.69135022e-04,   1.45347837e-03,\n",
       "          7.44312758e-04,   1.11341056e-03,   1.09383291e-04,\n",
       "          3.82757719e-04,   2.77243854e-04,   4.03710063e-04,\n",
       "          2.46271394e-04,   2.73456811e-04,   3.50861395e-04,\n",
       "          5.42006103e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   2.44528671e-05,\n",
       "          5.74594995e-04,   1.00818565e-03,   4.38504485e-04,\n",
       "          6.34438034e-04,   4.89214437e-05,   2.30634044e-04,\n",
       "          7.73267623e-05,   8.29713340e-05,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
